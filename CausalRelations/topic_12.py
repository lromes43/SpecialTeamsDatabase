# -*- coding: utf-8 -*-
"""Topic 12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19kkeJTIMnHgTwnvsVLWCGXoeOZUHqTyO

# DAGs for Causal Inference

This Colab notebook introduces **Directed Acyclic Graphs (DAGs)** and how they help us reason about causality.

**What you'll do today**
- Start with the **College (Education) â†’ Earnings** example (with simulated data)
- Understand **confounding** (backdoor paths)
- Explore **collider/selection bias** with the **Movie Star** example
- Run regressions and interpret differences when (not) controlling for confounders
"""

import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
import plotnine as p

# For reproducibility
np.random.seed(70200)

"""
## 1) College (Education) â†’ Earnings

We want the causal effect of **Education** on **Earnings**. Two key **confounders**:

- **Parental Income (PI)**: affects both Education and Earnings  
- **Ability (A)**: affects both Education and Earnings  

We'll simulate some data consistent with this story.
"""

# Simulate mock data
n = 2000
parent_income = np.random.normal(50, 15, n)
ability = np.random.normal(0, 1, n)
education = 0.30*parent_income + 1.50*ability + np.random.normal(0,5,n)
earnings = 2.00*education + 1.00*parent_income + 3.00*ability + np.random.normal(0,10,n)

edu_df = pd.DataFrame({
    "earnings": earnings,
    "education": education,
    "parent_income": parent_income,
    "ability": ability
})
edu_df.head()

"""
### Compare regressions

- **Naive**: regress Earnings on Education only (likely **biased**).  
- **Control for PI**: partial control, still omits Ability.  
- **Control for PI & Ability**: blocks both backdoor paths â†’ closer to causal effect.
"""

# sm.OLS.from_formula is nother way to run sm.OLS function

m1 = sm.OLS.from_formula("earnings ~ education", data=edu_df).fit()
m2 = sm.OLS.from_formula("earnings ~ education + parent_income", data=edu_df).fit()
m3 = sm.OLS.from_formula("earnings ~ education + parent_income + ability", data=edu_df).fit()

# Summarize side by side using summary_col
from statsmodels.iolib.summary2 import summary_col

results_table = summary_col([m1,m2,m3],
                            stars=True,
                            model_names=["Naive","Control PI","Control PI & Ability"],
                            info_dict={"R2":lambda x: f"{x.rsquared:.2f}",
                                       "N":lambda x: f"{int(x.nobs)}"})
print(results_table)

"""**Interpretation tip**  
If the **education** coefficient changes a lot after adding **parental_income** and **ability**, that signals potential confounding.

## 2) Colliders & Selection Bias: The Movie Star Example

**Idea**: In the full population, **Beauty** and **Talent** are independent.  
But if we **select** only people with high *(Beauty + Talent)* (i.e., "movie stars"), we **condition on a collider** and induce a negative correlation.

**DAG intuition**: `Beauty â†’ Star â† Talent`  
- `Star` is a **collider** (two arrows collide).  
- By default, Beauty and Talent are independent.  
- Conditioning on **Star** (e.g., analyzing only stars) **creates a spurious correlation**.
"""

# Generate synthetic population
stars = pd.DataFrame({
    "beauty": np.random.normal(size=2500),
    "talent": np.random.normal(size=2500),
})
stars["score"] = stars["beauty"] + stars["talent"]
c85 = np.percentile(stars["score"], 85)
stars["star"] = (stars["score"] > c85).astype(int)

stars.head()

"""### Visualize the relationships"""

# Population scatter
(
    p.ggplot(stars, p.aes(x="talent", y="beauty"))
    + p.geom_point(size=0.5, alpha=0.5)
    + p.labs(title="Beauty vs Talent: Full Population")
)

# Among stars only (selection = collider conditioning)
(
    p.ggplot(stars[stars.star == 1], p.aes(x="talent", y="beauty"))
    + p.geom_point(size=0.5, alpha=0.5, color="red")
    + p.labs(title="Beauty vs Talent: Among 'Stars' (Top 15%)")
)

# Among non-stars
(
    p.ggplot(stars[stars.star == 0], p.aes(x="talent", y="beauty"))
    + p.geom_point(size=0.5, alpha=0.5, color="blue")
    + p.labs(title="Beauty vs Talent: Among Non-Stars")
)

"""### Regression comparison"""

pop = sm.OLS.from_formula("beauty ~ talent + star", data=stars).fit()
print(pop.summary())

pop = sm.OLS.from_formula("beauty ~ talent", data=stars).fit()
sel = sm.OLS.from_formula("beauty ~ talent", data=stars[stars.star==1]).fit()
nsel = sm.OLS.from_formula("beauty ~ talent", data=stars[stars.star==0]).fit()

results_table2 = summary_col([pop, sel, nsel],
                             stars=True,
                             model_names=["Population","Stars Only","Non-Stars"],
                             info_dict={"R2":lambda x: f"{x.rsquared:.2f}",
                                        "N":lambda x: f"{int(x.nobs)}"})
print(results_table2)

"""ðŸ§  **Key lesson**  
- In the **population**, Beauty and Talent are independent (coef â‰ˆ 0).  
- **Conditioning on Star** induces a **spurious negative** relationship.  
- Selection bias is often collider bias in disguise.

## 4) Wrap-up
- **Confounders**: open backdoor paths; control for them.  
- **Colliders**: block paths; do **not** control for them.  
- **Selection** often acts like a collider.  
- Simulating data is a great way to build intuition about bias.
"""