library(class)
library(class)
library(caret)
library(class)
library(caret)
library(ISLR)
library(ggplot2)
library(lattice)
summary(Default)
norm_model <- preProcess(Default, method = c('range'))
default_normalized<- predict(norm_model, Default)
View(default_normalized)
default_normalized<- default_normalized[, -2]
index_train <- createDataPartition(default_normalized$default, p=0.8, list = false)
index_train <- createDataPartition(default_normalized$default, p=0.8, list = FALSE)
train <- default_normalized[index_train,]
test<- default_normalized[-index_train,]
View(default_normalized)
train_predictor<- train[,2:3]
test_predictor <- test [,2:3]
predicted<- knn(train_predictor, test_predictor, cl = train_label, k=4)
predicted<- knn(train_predictor, test_predictor, cl=train_label, k=4)
train_label <-train[,1]
test_label <- test [,1]
predicted<- knn(train_predictor, test_predictor, cl=train_label, k=4)
head(predicted)
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
n <- 100
x <- runif(n, min=0, max =100)
y <- 50* x + 100 + rnorm(n, mean = 0, sd = 10)
set.seed(42)
n <- 100
x <- runif(n, min=0, max =100)
y <- 50* x + 100 + rnorm(n, mean = 0, sd = 10)
m <- 0
b <- 0
alpha <- 0.00001
iterations <- 1000
set.seed(42)
n <- 100
x <- runif(n, min=0, max =100)
y <- 50* x + 100 + rnorm(n, mean = 0, sd = 10)
m <- 0
b <- 0
alpha <- 0.00001
iterations <- 1000
gradient_descent <- function(x,y,b,alpha, iterations){
n <-length (y)
cost_history <- numeric(iterations)
for (i in 1:iterations){
y_pred <- m*x+b
gradient_m <- -(2/n)* sum(x*(y-y_pred))
gradient_b <- -(2/n) * sum(y - y_pred)
m <- m - alpha * gradient_m
b <- b - alpha * gradient_b
cost <- sum((y-y_pred)^2)/n
cost_history[i] <- cost
if (i %% 100 == 0){
cat("Iteration:", i, "Cost:", cost, "\n")
}
}
return(list(m=m, b=b, cost_history = cost_history))
}
result <- gradient_descent(x,y,m,b,alpha, iterations)
gradient_descent <- function(x,y,b,alpha, iterations){
n <-length (y)
cost_history <- numeric(iterations)
for (i in 1:iterations){
y_pred <- m*x+b
gradient_m <- -(2/n)* sum(x*(y-y_pred))
gradient_b <- -(2/n) * sum(y - y_pred)
m <- m - alpha * gradient_m
b <- b - alpha * gradient_b
cost <- sum((y-y_pred)^2)/n
cost_history[i] <- cost
if (i %% 100 == 0){
cat("Iteration:", i, "Cost:", cost, "\n")
}
}
return(list(m=m, b=b, cost_history = cost_history))
}
result <- gradient_descent(x,y,m,b,alpha, iterations)
gradient_descent <- function(x,y,b,alpha, iterations){
n <-length (y)
cost_history <- numeric(iterations)
for (i in 1:iterations){
y_pred <- m*x+b
gradient_m <- -(2/n)* sum(x*(y-y_pred))
gradient_b <- -(2/n) * sum(y - y_pred)
m <- m - alpha * gradient_m
b <- b - alpha * gradient_b
cost <- sum((y-y_pred)^2)/n
cost_history[i] <- cost
if (i %% 100 == 0){
cat("Iteration:", i, "Cost:", cost, "\n")
}
}
return(list(m=m, b=b, cost_history = cost_history))
}
result <- gradient_descent(x,y,m,b,alpha,iterations)
gradient_descent <- function(x,y,m,b,alpha, iterations){
n <-length (y)
cost_history <- numeric(iterations)
for (i in 1:iterations){
y_pred <- m*x+b
gradient_m <- -(2/n)* sum(x*(y-y_pred))
gradient_b <- -(2/n) * sum(y - y_pred)
m <- m - alpha * gradient_m
b <- b - alpha * gradient_b
cost <- sum((y-y_pred)^2)/n
cost_history[i] <- cost
if (i %% 100 == 0){
cat("Iteration:", i, "Cost:", cost, "\n")
}
}
return(list(m=m, b=b, cost_history = cost_history))
}
result <- gradient_descent(x,y,m,b,alpha,iterations)
gradient_descent <- function(x,y,m,b,alpha, iterations){
n <-length (y)
cost_history <- numeric(iterations)
for (i in 1:iterations){
y_pred <- m*x+b
gradient_m <- -(2/n)* sum(x*(y-y_pred))
gradient_b <- -(2/n) * sum(y - y_pred)
m <- m - alpha * gradient_m
b <- b - alpha * gradient_b
cost <- sum((y-y_pred)^2)/n
cost_history[i] <- cost
if (i %% 100 == 0){
cat("Iteration:", i, "Cost:", cost, "\n")
}
}
return(list(m=m, b=b, cost_history = cost_history))
}
result <- gradient_descent(x,y,m,b,alpha,iterations)
final_m <- result$m
final_b _ result$b
gradient_descent <- function(x,y,m,b,alpha, iterations){
n <-length (y)
cost_history <- numeric(iterations)
for (i in 1:iterations){
y_pred <- m*x+b
gradient_m <- -(2/n)* sum(x*(y-y_pred))
gradient_b <- -(2/n) * sum(y - y_pred)
m <- m - alpha * gradient_m
b <- b - alpha * gradient_b
cost <- sum((y-y_pred)^2)/n
cost_history[i] <- cost
if (i %% 100 == 0){
cat("Iteration:", i, "Cost:", cost, "\n")
}
}
return(list(m=m, b=b, cost_history = cost_history))
}
result <- gradient_descent(x,y,m,b,alpha,iterations)
final_m <- result$m
final_b <- result$b
cost_history <- result$cost_history
print(cost_history)
plot(x,y, main " Gradient Descent Fitted Line", xlab = "x", ylab = "y")
plot(x,y, main = " Gradient Descent Fitted Line", xlab = "x", ylab = "y")
abline(a = final_b, b =final_m, col = "blue")
plot(1: iterations, cost_history, type = "1", col = "blue", lwd = 2,
main = "cost Function ovder iterations", xlab = "iterations", ylab = "cost")
plot(1: iterations, cost_history, type = "l", col = "blue", lwd = 2,
main = "cost Function ovder iterations", xlab = "iterations", ylab = "cost")
plot(1: iterations, cost_history, type = "l", col = "blue", lwd = 2,
main = "Cost Function over iterations", xlab = "iterations", ylab = "cost")
knitr::opts_chunk$set(echo = TRUE)
print(result$m)
Adjusted_Slope <- print(result$m)
Adjusted_Intercept <- print(result$b)
Adjusted_Slope <- print(result$m)
Adjusted_Intercept <- print(result$b)
AdjustedCostHistory <- print(result$cost_history)
Adjusted_Slope <- print(result$m)
Adjusted_Intercept <- print(result$b)
knitr::opts_chunk$set(echo = TRUE)
w <- c(0.4430031,0.4430031)
b <- 0
dim(HousingData)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/Personal/R/Linear Regression")
HousingData <- read.csv("PortlandOregonHousing.csv")
summary(HousingData)
dim(HousingData)
HousingDataNorm <- scale(HousingData)
plot(x= HousingDataNorm[,1], y = HousingDataNorm[,3],
xlab = "Square Feet",
ylab = "Price",
main = "Sq Ft vs Price"
)
plot(x=HousingDataNorm[,2], y= HousingDataNorm[,3],
xlab = "Bedrooms",
ylab = "Price",
main = "Bedrooms vs Price" )
HousingDataNorm <- data.frame(HousingDataNorm)
plot_ly(data = HousingDataNorm, x= ~HousingDataNorm[,1], y= ~HousingDataNorm[,2], z= ~HousingDataNorm[,3],
type = "scatter3d", mode = "markers",
marker = list( size = 4, color = ~ HousingDataNorm[,3], colorscale = "virdis"))
library(plotly)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/Personal/R/Linear Regression")
HousingData <- read.csv("PortlandOregonHousing.csv")
summary(HousingData)
dim(HousingData)
HousingDataNorm <- scale(HousingData)
plot(x= HousingDataNorm[,1], y = HousingDataNorm[,3],
xlab = "Square Feet",
ylab = "Price",
main = "Sq Ft vs Price"
)
plot(x=HousingDataNorm[,2], y= HousingDataNorm[,3],
xlab = "Bedrooms",
ylab = "Price",
main = "Bedrooms vs Price" )
HousingDataNorm <- data.frame(HousingDataNorm)
plot_ly(data = HousingDataNorm, x= ~HousingDataNorm[,1], y= ~HousingDataNorm[,2], z= ~HousingDataNorm[,3],
type = "scatter3d", mode = "markers",
marker = list( size = 4, color = ~ HousingDataNorm[,3], colorscale = "virdis"))
x1 <- HousingDataNorm$SquareFeet #feature 1
x2 <- HousingDataNorm$Bedrooms #feature 2
y <- HousingDataNorm$Price #label
alpha <- 0.0016 #learning rate /steps
iterations <- 500 #number of steps
m1 <- 0 #parameter 1
m2 <- 0 #paramter 2
b<- 0
HousingGradientDescent <- function (x1,x2,y,alpha,iterations,m1,m2){ ##creates function called gradient descent
n <- length (y) ##defines nunber of records from the length of records in dataframe
costhistory <- numeric(iterations) ##creates variable called costhistory which stores the number of iterations from 0-1000
for (i in 1:iterations){ ##loop from 1 to 1000 iterations
y_pred <- m1*x1 + m2*x2 ##prediction of label (cost of house) using two features multiplied by coefficients
M_Gradient <- -(2/n) * sum(x1*x2 * (y-y_pred)) #derivative of the coefficients
B_Gradient <- -(2/n) *sum(y-y_pred) #derivative of initial value
m1 <- m1 - alpha * M_Gradient ##readjusts the coefficeints
m2 <- m2 - alpha * M_Gradient
B<- b - alpha * B_Gradient ##readjusts the inital value
cost <- sum((y- y_pred)^2)/n ##Mean of Sum Squares
costhistory[i] <- cost ##Stores Cost Functions so easily indexable
if ( i %% 15 == 0){ ##if through 15 iterations and 0 left over print a string with these values
cat("iteration", i, "cost", cost, "\n")
}
}
return(list(m1=m1,m2 = m2, b=b, costhistory = costhistory)) ## returns values
}
GradientDescentResult <- HousingGradientDescent(x1,x2,y,alpha,iterations,m1,m2)
print(GradientDescentResult$m1)
print(GradientDescentResult$m2)
print(GradientDescentResult$b)
final_cost <- tail(GradientDescentResult$costhistory,1)
print(final_cost)
plot(
GradientDescentResult$costhistory, type = "l", col = "blue",
main = "Cost Over Iterations",
xlab = "Iterations",
ylab = "cost"
)
(92982322850-0.4529882 /92982322850 ) *100 ##reduced error by over 9 quadrillion %
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/Personal/R/Linear Regression")
HousingData <- read.csv("PortlandOregonHousing.csv")
summary(HousingData)
dim(HousingData)
HousingDataNorm <- scale(HousingData)
plot(x= HousingDataNorm[,1], y = HousingDataNorm[,3],
xlab = "Square Feet",
ylab = "Price",
main = "Sq Ft vs Price"
)
plot(x=HousingDataNorm[,2], y= HousingDataNorm[,3],
xlab = "Bedrooms",
ylab = "Price",
main = "Bedrooms vs Price" )
HousingDataNorm <- data.frame(HousingDataNorm)
plot_ly(data = HousingDataNorm, x= ~HousingDataNorm[,1], y= ~HousingDataNorm[,2], z= ~HousingDataNorm[,3],
type = "scatter3d", mode = "markers",
marker = list( size = 4, color = ~ HousingDataNorm[,3], colorscale = "virdis"))
x1 <- HousingDataNorm$SquareFeet #feature 1
x2 <- HousingDataNorm$Bedrooms #feature 2
y <- HousingDataNorm$Price #label
alpha <- 0.0001 #learning rate /steps
iterations <- 10000 #number of steps
m1 <- 0.7 #parameter 1
m2 <- 0.073 #paramter 2
b<- 0
HousingGradientDescent <- function (x1,x2,y,alpha,iterations,m1,m2){ ##creates function called gradient descent
n <- length (y) ##defines nunber of records from the length of records in dataframe
costhistory <- numeric(iterations) ##creates variable called costhistory which stores the number of iterations from 0-1000
for (i in 1:iterations){ ##loop from 1 to 1000 iterations
y_pred <- m1*x1 + m2*x2 ##prediction of label (cost of house) using two features multiplied by coefficients
M_Gradient <- -(2/n) * sum(x1*x2 * (y-y_pred)) #derivative of the coefficients
B_Gradient <- -(2/n) *sum(y-y_pred) #derivative of initial value
m1 <- m1 - alpha * M_Gradient ##readjusts the coefficeints
m2 <- m2 - alpha * M_Gradient
B<- b - alpha * B_Gradient ##readjusts the inital value
cost <- sum((y- y_pred)^2)/n ##Mean of Sum Squares
costhistory[i] <- cost ##Stores Cost Functions so easily indexable
if ( i %% 15 == 0){ ##if through 15 iterations and 0 left over print a string with these values
cat("iteration", i, "cost", cost, "\n")
}
}
return(list(m1=m1,m2 = m2, b=b, costhistory = costhistory)) ## returns values
}
GradientDescentResult <- HousingGradientDescent(x1,x2,y,alpha,iterations,m1,m2)
print(GradientDescentResult$m1)
print(GradientDescentResult$m2)
print(GradientDescentResult$b)
final_cost <- tail(GradientDescentResult$costhistory,1)
print(final_cost)
plot(
GradientDescentResult$costhistory, type = "l", col = "blue",
main = "Cost Over Iterations",
xlab = "Iterations",
ylab = "cost"
)
(92982322850-0.4529882 /92982322850 ) *100 ##reduced error by over 9 quadrillion %
setwd("~/Desktop/Personal/Football Data")
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/Personal/Football Data")
Punt <- read.csv ("PuntDataML.numbers")
View(Punt)
setwd("~/Desktop/Personal/Football Data")
Punt <- read.csv ("PuntDataML.csv")
setwd("~/Desktop/Personal/Football Data")
Punt <- read.csv ("PuntDataML.csv")
head(Punt)
View(Punt)
frame(Punt)
df = subset(punt, select = -c(,15))
df <- subset(Punt, select = -c(15))
df <- subset(Punt, select = -c(15))
is.data.frame(df)
view(df)
View(df)
df <- subset(Punt, select = -c(1,15))
is.data.frame(df)
View(df)
install.packages("RMySQL")
install.packages("RMariaDB")
library(RMariaDB)
mysqlconnection = dbConnect(RMySQL:: MySQL(),
dbname = 'Football',
host = 'localhost',
port = 3306,
user = 'Lromes'
password = '1234!')
mysqlconnection = dbConnect(RMySQL:: MySQL(),
dbname = 'Football',
host = 'localhost',
port = 3306,
user = 'Lromes'
password = 1234!)
mysqlconnection = dbConnect(RMySQL:: MySQL(),
dbname = 'Football',
host = 'localhost',
port = 3306,
user = 'Lromes',
password = 1234!)
mysqlconnection = dbConnect(RMySQL:: MySQL(),
dbname = 'Football',
host = 'localhost',
port = 3306,
user = 'Lromes',
password = '1234!')
dbListTables(mysqlconnection)
result = dbSendQuery(mysqlconnection, "Select * from kickoff")
#store the result of query
data.frame = fetch(result, n =5)
result = dbSendQuery(mysqlconnection, "Select * from kickoff")
#store the result of query
data.frame = dbFetch(result, n =5)
View(data.frame)
result = dbSendQuery(mysqlconnection, "Select * from kickoff")
result = dbSendQuery(mysqlconnection, "Select * from kickoff")
KOMeta = dbSendQuery(mysqlconnection, "Describe Kickoff")
KOMeta = dbSendQuery(mysqlconnection, "Describe Kickoff")
dbClearResult(previous_result)
result = dbSendQuery(mysqlconnection, "Select * from kickoff")
dbClearResult(result)
result = dbSendQuery(mysqlconnection, "Select * from kickoff")
#store the result of query
df = dbFetch(result, n =5)
print(df)
dbClearResult(result)
#KOMeta = dbSendQuery(mysqlconnection, "Describe Kickoff")
#data.frame2 = dbFetch(KOMeta)
#print(data.frame2)
dbClearResult(KOMeta)
dbClearResult(result)
result = dbSendQuery(mysqlconnection, "Select * from kickoff")
#store the result of query
df = dbFetch(result, n =5)
print(df)
dbClearResult(result)
KOMeta = dbSendQuery(mysqlconnection, "Describe Kickoff")
data.frame2 = dbFetch(KOMeta)
print(data.frame2)
dbClearResult(KOMeta)
